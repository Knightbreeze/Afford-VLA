from types import SimpleNamespace

import cv2
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import torch

from sam3.model.geometry_encoders import Prompt
from sam3.model.sam3_image_processor import Sam3Processor

# å¯¼å…¥ SAM 3 ç›¸å…³æ¨¡å—
from sam3.model_builder import build_sam3_image_model


class CachedSAM3Inference:
    """
    SAM 3 é«˜æ•ˆæŽ¨ç†åŒ…è£…å™¨ï¼š
    1. encode_image(): åªè¿è¡Œä¸€æ¬¡é‡åž‹ Vision Encoderï¼Œç¼“å­˜ç‰¹å¾ã€‚
    2. predict(): æŽ¥æ”¶æ–‡æœ¬åˆ—è¡¨ï¼Œå¤ç”¨ç¼“å­˜ç‰¹å¾ï¼Œå¿«é€Ÿè§£ç ã€‚
    """

    def __init__(self, model, processor):
        self.model = model
        self.processor = processor
        self.device = model.device
        self.cached_backbone_out = None
        self.original_size = None  # (width, height)
        self.img_tensor = None

    def encode_image(self, image_path_or_pil):
        """
        ç¬¬ä¸€é˜¶æ®µï¼šå›¾åƒç¼–ç ï¼ˆæœ€è€—æ—¶æ­¥éª¤ï¼Œçº¦å  90% æ—¶é—´ï¼‰ã€‚
        åªéœ€è¿è¡Œä¸€æ¬¡ã€‚
        """
        # 1. åŠ è½½å›¾åƒ
        if isinstance(image_path_or_pil, str):
            image = Image.open(image_path_or_pil).convert("RGB")
        else:
            image = image_path_or_pil.convert("RGB")

        self.original_size = image.size  # (W, H)

        # å°è¯•è‡ªåŠ¨æ£€æµ‹æ¨¡åž‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
        target_size = 1024  # é»˜è®¤ SAM æ ‡å‡†
        try:
            # æ£€æŸ¥å¸¸è§çš„å±žæ€§å
            if hasattr(self.model.backbone, "img_size"):
                target_size = self.model.backbone.img_size
            elif hasattr(self.model, "image_encoder") and hasattr(self.model.image_encoder, "img_size"):
                target_size = self.model.image_encoder.img_size
            print(f"â„¹ï¸  Model expects input size: {target_size}x{target_size}")
        except Exception:
            pass

        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ processor æä¾›çš„ set_image
        # æˆ‘ä»¬ç§»é™¤ try-except ä»¥ä¾¿çœ‹åˆ° processor å†…éƒ¨çš„çœŸå®žé”™è¯¯ï¼ˆå¦‚æžœæœ‰ï¼‰
        try:
            inference_state = self.processor.set_image(image)
            if isinstance(inference_state, dict):
                for key in ["image_tensor", "img_tensor", "images", "input_tensor"]:
                    if key in inference_state and isinstance(inference_state[key], torch.Tensor):
                        self.img_tensor = inference_state[key].unsqueeze(0).to(self.device).contiguous()
                        print("âœ… Used processor.set_image() for preprocessing.")
                        break
        except Exception as e:
            print(f"âš ï¸  processor.set_image() failed or returned unknown format: {e}")
            inference_state = None

        # å¦‚æžœ processor å¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨é¢„å¤„ç†ï¼ˆæ ‡å‡† SAM é€»è¾‘ï¼šResize Longest + Padï¼‰
        if self.img_tensor is None:
            print(f"âš ï¸  Falling back to manual preprocessing (Target: {target_size}x{target_size})...")

            img_np = np.array(image)
            old_h, old_w = img_np.shape[:2]
            scale = target_size * 1.0 / max(old_h, old_w)
            new_h, new_w = int(old_h * scale), int(old_w * scale)

            # Resize longest side
            img_resized = cv2.resize(img_np, (new_w, new_h))

            # Pad to target_size (bottom-right padding)
            pad_h = target_size - new_h
            pad_w = target_size - new_w
            img_padded = np.pad(img_resized, ((0, pad_h), (0, pad_w), (0, 0)), mode="constant", constant_values=0)

            # Normalize & Convert
            img_tensor = torch.from_numpy(img_padded).float().permute(2, 0, 1).contiguous() / 255.0

            mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32, device=self.device).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32, device=self.device).view(3, 1, 1)

            img_tensor = img_tensor.to(self.device)
            img_tensor = (img_tensor - mean) / std
            self.img_tensor = img_tensor.unsqueeze(0).contiguous()

        # 3. è¿è¡Œ Vision Encoder (Backbone)
        self.model.eval()
        # å¼ºåˆ¶åŒæ­¥æ¨¡åž‹è®¾å¤‡
        self.model.to(self.device)

        with torch.no_grad():
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                backbone_out = {"img_batch_all_stages": self.img_tensor}

                # å†æ¬¡ç¡®ä¿ tensor åœ¨ device ä¸Š
                if self.img_tensor.device != self.device:
                    self.img_tensor = self.img_tensor.to(self.device)

                print(f"ðŸš€ Running backbone forward... Input shape: {self.img_tensor.shape}")
                image_feats = self.model.backbone.forward_image(self.img_tensor)
                backbone_out.update(image_feats)

                self.cached_backbone_out = backbone_out
                print("âœ… Image encoded successfully.")
            except AssertionError as e:
                print("\nâŒ AssertionError detected in model forward pass!")
                print(
                    "This usually means the input image size does not match the model's pre-computed positional embeddings."
                )
                print(f"Current Input Shape: {self.img_tensor.shape}")

                # å°è¯•æ£€æŸ¥æ¨¡åž‹å†…éƒ¨çš„ freqs_cis å½¢çŠ¶ä»¥å¸®åŠ©è°ƒè¯•
                try:
                    # æ·±åº¦ä¼˜å…ˆæœç´¢ freqs_cis
                    for name, module in self.model.named_modules():
                        if hasattr(module, "freqs_cis") and isinstance(module.freqs_cis, torch.Tensor):
                            print(f"Found internal 'freqs_cis' in {name}: shape={module.freqs_cis.shape}")
                            break
                except:
                    pass
                raise e  # filepath: /home/nightbreeze/research/Code/AffVLA/sam3/multi_prompt_inference.py

    # ...existing code...
    def encode_image(self, image_path_or_pil):
        """
        ç¬¬ä¸€é˜¶æ®µï¼šå›¾åƒç¼–ç ï¼ˆæœ€è€—æ—¶æ­¥éª¤ï¼Œçº¦å  90% æ—¶é—´ï¼‰ã€‚
        åªéœ€è¿è¡Œä¸€æ¬¡ã€‚
        """
        # 1. åŠ è½½å›¾åƒ
        if isinstance(image_path_or_pil, str):
            image = Image.open(image_path_or_pil).convert("RGB")
        else:
            image = image_path_or_pil.convert("RGB")

        self.original_size = image.size  # (W, H)

        # å°è¯•è‡ªåŠ¨æ£€æµ‹æ¨¡åž‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
        target_size = 1024  # é»˜è®¤ SAM æ ‡å‡†
        try:
            # æ£€æŸ¥å¸¸è§çš„å±žæ€§å
            if hasattr(self.model.backbone, "img_size"):
                target_size = self.model.backbone.img_size
            elif hasattr(self.model, "image_encoder") and hasattr(self.model.image_encoder, "img_size"):
                target_size = self.model.image_encoder.img_size
            print(f"â„¹ï¸  Model expects input size: {target_size}x{target_size}")
        except Exception:
            pass

        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ processor æä¾›çš„ set_image
        # æˆ‘ä»¬ç§»é™¤ try-except ä»¥ä¾¿çœ‹åˆ° processor å†…éƒ¨çš„çœŸå®žé”™è¯¯ï¼ˆå¦‚æžœæœ‰ï¼‰
        try:
            inference_state = self.processor.set_image(image)
            if isinstance(inference_state, dict):
                for key in ["image_tensor", "img_tensor", "images", "input_tensor"]:
                    if key in inference_state and isinstance(inference_state[key], torch.Tensor):
                        self.img_tensor = inference_state[key].unsqueeze(0).to(self.device).contiguous()
                        print("âœ… Used processor.set_image() for preprocessing.")
                        break
        except Exception as e:
            print(f"âš ï¸  processor.set_image() failed or returned unknown format: {e}")
            inference_state = None

        # å¦‚æžœ processor å¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨é¢„å¤„ç†ï¼ˆæ ‡å‡† SAM é€»è¾‘ï¼šResize Longest + Padï¼‰
        if self.img_tensor is None:
            print(f"âš ï¸  Falling back to manual preprocessing (Target: {target_size}x{target_size})...")

            img_np = np.array(image)
            old_h, old_w = img_np.shape[:2]
            scale = target_size * 1.0 / max(old_h, old_w)
            new_h, new_w = int(old_h * scale), int(old_w * scale)

            # Resize longest side
            img_resized = cv2.resize(img_np, (new_w, new_h))

            # Pad to target_size (bottom-right padding)
            pad_h = target_size - new_h
            pad_w = target_size - new_w
            img_padded = np.pad(img_resized, ((0, pad_h), (0, pad_w), (0, 0)), mode="constant", constant_values=0)

            # Normalize & Convert
            img_tensor = torch.from_numpy(img_padded).float().permute(2, 0, 1).contiguous() / 255.0

            mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32, device=self.device).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32, device=self.device).view(3, 1, 1)

            img_tensor = img_tensor.to(self.device)
            img_tensor = (img_tensor - mean) / std
            self.img_tensor = img_tensor.unsqueeze(0).contiguous()

        # 3. è¿è¡Œ Vision Encoder (Backbone)
        self.model.eval()
        # å¼ºåˆ¶åŒæ­¥æ¨¡åž‹è®¾å¤‡
        self.model.to(self.device)

        with torch.no_grad():
            try:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                backbone_out = {"img_batch_all_stages": self.img_tensor}

                # å†æ¬¡ç¡®ä¿ tensor åœ¨ device ä¸Š
                if self.img_tensor.device != self.device:
                    self.img_tensor = self.img_tensor.to(self.device)

                print(f"ðŸš€ Running backbone forward... Input shape: {self.img_tensor.shape}")
                image_feats = self.model.backbone.forward_image(self.img_tensor)
                backbone_out.update(image_feats)

                self.cached_backbone_out = backbone_out
                print("âœ… Image encoded successfully.")
            except AssertionError as e:
                print("\nâŒ AssertionError detected in model forward pass!")
                print(
                    "This usually means the input image size does not match the model's pre-computed positional embeddings."
                )
                print(f"Current Input Shape: {self.img_tensor.shape}")

                # å°è¯•æ£€æŸ¥æ¨¡åž‹å†…éƒ¨çš„ freqs_cis å½¢çŠ¶ä»¥å¸®åŠ©è°ƒè¯•
                try:
                    # æ·±åº¦ä¼˜å…ˆæœç´¢ freqs_cis
                    for name, module in self.model.named_modules():
                        if hasattr(module, "freqs_cis") and isinstance(module.freqs_cis, torch.Tensor):
                            print(f"Found internal 'freqs_cis' in {name}: shape={module.freqs_cis.shape}")
                            break
                except:
                    pass
                raise e
            except Exception as e:
                print(f"Error in backbone forward: {e}")
                raise


def predict(self, prompt_list, conf_threshold=0.2):
    def predict(self, prompt_list, conf_threshold=0.2):
        """
        ç¬¬äºŒé˜¶æ®µï¼šå¤š Prompt è§£ç ï¼ˆéžå¸¸å¿«ï¼‰ã€‚
        å¯ä»¥ä¸€æ¬¡æ€§ä¼ å…¥å¤šä¸ª promptã€‚
        """
        if self.cached_backbone_out is None:
            raise ValueError("Please run encode_image() first.")

        all_results = []

        # 1. æ‰¹é‡å¤„ç†æ–‡æœ¬ç¼–ç  (Text Encoding)
        # å°†æ‰€æœ‰ prompt ä¸€æ¬¡æ€§ç¼–ç 
        # SAM3 backbone.forward_text æŽ¥æ”¶ list of strings
        with torch.no_grad():
            text_outputs = self.model.backbone.forward_text(prompt_list, device=self.device)

            # 2. é’ˆå¯¹æ¯ä¸ª Prompt è¿›è¡Œè§£ç 
            # æ³¨æ„ï¼šè™½ç„¶ Text Encoder å¯ä»¥ Batchï¼Œä½† Decoder é€šå¸¸éœ€è¦é€ä¸ª Prompt å¤„ç†
            # æˆ–è€…æˆ‘ä»¬éœ€è¦æž„é€ å¤æ‚çš„ Batched Inputã€‚ä¸ºäº†ä»£ç æ¸…æ™°ï¼Œè¿™é‡Œé€ä¸ªå¾ªçŽ¯ï¼Œä½†å¤ç”¨ Image Feature

            for i, prompt_text in enumerate(prompt_list):
                # æž„é€ å½“å‰ Prompt çš„ backbone_out
                # æˆ‘ä»¬éœ€è¦ä»Ž text_outputs ä¸­åˆ‡ç‰‡å‡ºç¬¬ i ä¸ªæ–‡æœ¬çš„ç‰¹å¾
                # text_outputs['language_features'] shape: [B, L, C]

                # æµ…æ‹·è´ç¼“å­˜ï¼Œé¿å…ä¿®æ”¹
                current_backbone_out = self.cached_backbone_out.copy()
                # æ›´æ–°æ–‡æœ¬ç‰¹å¾ï¼ˆè¿™é‡Œç›´æŽ¥å¼•ç”¨å…¨éƒ¨ï¼Œé€šè¿‡ text_ids é€‰æ‹©ï¼‰
                current_backbone_out.update(text_outputs)

                # 3. æž„é€  Input å¯¹è±¡ (Mock)
                find_input = SimpleNamespace(
                    img_ids=torch.tensor([0], device=self.device),
                    text_ids=torch.tensor([i], device=self.device),  # æŒ‡å‘å½“å‰ prompt
                    input_boxes=None,
                    input_boxes_mask=None,
                    input_boxes_label=None,
                    input_points=None,
                )

                # 4. æž„é€ ç©ºçš„å‡ ä½•æç¤º (Geometric Prompt)
                geometric_prompt = Prompt(box_embeddings=None, box_mask=None, box_labels=None)

                # 5. è¿è¡Œ Grounding (Fusion + Decoder)
                # è¿™æ­¥éžå¸¸å¿«ï¼Œå› ä¸ºå®ƒåªè¿è¡Œ Transformer Decoder
                out = self.model.forward_grounding(
                    backbone_out=current_backbone_out,
                    find_input=find_input,
                    find_target=None,
                    geometric_prompt=geometric_prompt,
                )

                # 6. æå–ç»“æžœ
                pred_masks = out["pred_masks"][0]  # [N, H, W]
                pred_scores = out["pred_logits"][0].sigmoid().squeeze(-1)  # [N]
                pred_boxes = out["pred_boxes"][0]  # [N, 4] (cx, cy, w, h) normalized

                # 7. è¿‡æ»¤ä½Žç½®ä¿¡åº¦ç»“æžœ
                keep_indices = pred_scores > conf_threshold
                if keep_indices.sum() > 0:
                    valid_masks = pred_masks[keep_indices]
                    valid_scores = pred_scores[keep_indices]
                    valid_boxes = pred_boxes[keep_indices]

                    # Resize masks åˆ°åŽŸå›¾å¤§å°
                    valid_masks = torch.nn.functional.interpolate(
                        valid_masks.unsqueeze(1),
                        size=(self.original_size[1], self.original_size[0]),
                        mode="bilinear",
                        align_corners=False,
                    ).squeeze(1)
                    valid_masks = (valid_masks > 0.0).cpu().numpy()

                    all_results.append(
                        {
                            "prompt": prompt_text,
                            "scores": valid_scores.cpu().numpy(),
                            "masks": valid_masks,
                            "boxes": valid_boxes.cpu().numpy(),  # normalized format
                        }
                    )
                else:
                    print(f"  - No object found for prompt: '{prompt_text}'")

        return all_results


def visualize_multi_results(image_path, results, output_path="output_multi_prompt.jpg"):
    """å¯è§†åŒ–å¤šä¸ªç±»åˆ«çš„ç»“æžœå¹¶ä¿å­˜"""
    image = Image.open(image_path).convert("RGB")
    plt.figure(figsize=(12, 8))
    plt.imshow(image)
    ax = plt.gca()

    colors = ["red", "green", "blue", "yellow", "cyan", "magenta"]

    for i, res in enumerate(results):
        color = colors[i % len(colors)]
        prompt = res["prompt"]

        for box, score, mask in zip(res["boxes"], res["scores"], res["masks"]):
            # ç»˜åˆ¶ Mask (åŠé€æ˜Ž)
            # åˆ›å»ºå½©è‰² mask å±‚
            mask_img = np.zeros((mask.shape[0], mask.shape[1], 4))
            mask_img[mask, :] = list(plt.cm.colors.to_rgba(color))
            mask_img[mask, 3] = 0.4  # Alpha
            ax.imshow(mask_img)

            # ç»˜åˆ¶ Box (cxcywh -> xyxy è½¬æ¢å¹¶åå½’ä¸€åŒ–)
            w, h = image.size
            cx, cy, bw, bh = box
            x1 = (cx - bw / 2) * w
            y1 = (cy - bh / 2) * h
            rect = plt.Rectangle((x1, y1), bw * w, bh * h, fill=False, edgecolor=color, linewidth=2)
            ax.add_patch(rect)

            # ç»˜åˆ¶æ ‡ç­¾
            ax.text(x1, y1, f"{prompt}: {score:.2f}", fontsize=10, bbox=dict(facecolor=color, alpha=0.5), color="white")

    plt.axis("off")
    plt.tight_layout()
    plt.savefig(output_path)
    print(f"Visualization saved to {output_path}")
    # plt.show() # å¦‚æžœåœ¨æ— å¤´æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œè¯·æ³¨é‡ŠæŽ‰æ­¤è¡Œ


# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # 1. è®¾ç½®è®¾å¤‡å’ŒåŠ è½½æ¨¡åž‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Loading SAM 3 model on {device}...")

    model = build_sam3_image_model().to(device)
    processor = Sam3Processor(model)

    # 2. åˆå§‹åŒ–ç¼“å­˜æŽ¨ç†å™¨
    cache_infer = CachedSAM3Inference(model, processor)

    # 3. è¾“å…¥é…ç½®
    IMAGE_PATH = "sam3/input/test1.png"  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„

    # å®šä¹‰å¤šä¸ª Prompt (è¿™å°±æ˜¯ä½ æƒ³è¦çš„â€œå¤šç±»åˆ«â€è¾“å…¥)
    # ä¾‹å¦‚ï¼šåˆ†è§£åŽçš„æŒ‡ä»¤ ["spoon handle", "cup rim", "spoon bowl"]
    PROMPTS = ["spoon handle", "cup", "spoon"]

    print(f"\nProcessing image: {IMAGE_PATH}")
    print(f"Prompts: {PROMPTS}")

    # 4. è¿è¡ŒæŽ¨ç†æµç¨‹
    # Step A: ç¼–ç å›¾åƒ (åªè¿è¡Œä¸€æ¬¡ï¼Œè€—æ—¶å¤§å¤´)
    cache_infer.encode_image(IMAGE_PATH)

    # Step B: é¢„æµ‹æ‰€æœ‰ Prompt (è¿è¡Œå¤šæ¬¡ï¼Œä½†éžå¸¸å¿«)
    # ä½ å¯ä»¥åœ¨è¿™é‡Œä¼ å…¥æ•´ä¸ªåˆ—è¡¨
    results = cache_infer.predict(PROMPTS, conf_threshold=0.25)

    # 5. è¾“å‡ºå’Œå¯è§†åŒ–
    print(f"\nFound objects for {len(results)}/{len(PROMPTS)} prompts.")
    for res in results:
        print(f"  Prompt '{res['prompt']}': found {len(res['scores'])} instances. Max score: {res['scores'].max():.2f}")

    if len(results) > 0:
        visualize_multi_results(IMAGE_PATH, results)
